{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys, datetime, logging\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import calendar\n",
    "from datetime import timedelta\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.getcwd().split('paper_std')[0])\n",
    "warnings.filterwarnings('ignore')\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "from util.s3_method import *\n",
    "from util.load_s3_data import LoadS3Data\n",
    "from util.time_method import *\n",
    "from util.plot_method import easy_plot,timeline_sample_kv, get_plot_diff_data\n",
    "from util.date import tz, pre_quarter_friday, next_quarter_friday\n",
    "from util.hedge_log import initlog\n",
    "from util.recover_depth import recoveryDepth\n",
    "from util.statistic_method_v2 import describe_series\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from util.Future_Load import *\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "from numpy.lib.stride_tricks import as_strided as stride\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = 'binance'\n",
    "symbol = 'btc_usdt'\n",
    "begin_time = datetime.datetime(2023, 12, 17, 1,tzinfo=TZ_8)\n",
    "end_time = datetime.datetime(2023, 12, 17, 2,tzinfo=TZ_8)\n",
    "\n",
    "ticker_data = LoadS3Data.get_cex_ticker(begin_time, end_time, symbol, exchange)\n",
    "trade_data = LoadS3Data.get_cex_trade(begin_time, end_time, exchange, symbol)\n",
    "depth_data = LoadS3Data.get_cex_depth_online(begin_time, end_time, exchange, symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ticker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllType_Data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mticker\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrade\u001b[39m\u001b[38;5;124m'\u001b[39m: trade, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: depth},f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ticker' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('AllType_Data.pkl', 'wb') as f:\n",
    "    pickle.dump({'ticker': ticker, 'trade': trade, 'depth': depth},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "1. The out-of-sample R-squared for 5-second returns is 10.5%, the direction of the next order is 64% accurate, and the out-of-sample R-squared for predicting the next 10 trade intervals is 9.8%. Important predictors of returns and direction are order book imbalance, recent transaction imbalance,and past trade returns, but the statistic obtained from recent trade volume is more effective for the prediction of duration\n",
    "2. Transaction data is the most valuable, and tuning the reference and introducing other stocks brings a non-significant improvement in results\n",
    "3. Predictable span is very short, only predictable before 3min, 2000 transactions, 500K volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First: Data Preparation**\n",
    "\n",
    "The transaction and quote update data are merged based on their timestamps. The best bid and ask information of trades are determined by the most recent quote information as of the time of the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(start_date, end_date, exchange, symbol, plot_interval_us):\n",
    "    trade = FutureData.thread_get(FutureData.get_cex_trade, start_date, end_date, symbol, exchange, plot_interval_us = plot_interval_us)\n",
    "    ticker = FutureData.thread_get(FutureData.get_cex_ticker, start_date, end_date, symbol, exchange, plot_interval_us = plot_interval_us)\n",
    "    trade = pd.DataFrame(trade)\n",
    "    ticker = pd.DataFrame(ticker).rename(columns={'tp':'time','ap':'ask1','aa':'askqty1','bp':'bid1','ba':'bidqty1'})\n",
    "    trade = trade[['T','p','q','m']].rename(columns = {'p':'price','q':'qty','T':'time','m':'is_buyer_maker'})\n",
    "    return ticker[['time', 'ask1', 'askqty1', 'bid1', 'bidqty1']], trade\n",
    "\n",
    "def merge_data(ticker,trade):\n",
    "    trade = trade.sort_values('time').reset_index(drop = True)\n",
    "    trade['transaction'] = 1\n",
    "    trade['volume'] = trade['qty']\n",
    "    trade[['transaction', 'volume']] = trade[['transaction', 'volume']].cumsum()\n",
    "    data = pd.concat([ticker, trade], ignore_index = True)\n",
    "    data = data.sort_values(['time', 'transaction']).reset_index(drop = True)\n",
    "    del ticker, trade\n",
    "    data[['ask1', 'bid1', 'price', 'transaction', 'volume', 'is_buyer_maker', 'askqty1', 'bidqty1', 'qty']] = \\\n",
    "        data[['ask1', 'bid1', 'price', 'transaction', 'volume', 'is_buyer_maker', 'askqty1', 'bidqty1', 'qty']].fillna(method = 'ffill')\n",
    "    data = data.dropna(subset = ['ask1'])\n",
    "    return data.reset_index(drop = True)\n",
    "\n",
    "\n",
    "start_date = datetime.datetime(2024,2,5,0,tzinfo=TZ_8)\n",
    "end_date = datetime.datetime(2024,2,5,1, tzinfo=TZ_8)\n",
    "exchange, symbol =\"binance\", \"btc_usdt\"\n",
    "plot_interval_us = 100000\n",
    "ticker,trade = get_data(start_date, end_date, exchange, symbol, plot_interval_us)\n",
    "# [ticker, trade] = load('data/btc.pkl')\n",
    "# data = merge_data(ticker, trade)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second: Response Variables**\n",
    "\n",
    "* **Part A:** Time clocks (Achieved)\n",
    "\n",
    "The author uses three kinds of time clock, which are **calendar clock, transcation clock and volume clock** respectively. With a starting timestamp T, a span ∆ > 0 and a clock mode M ∈ {calendar, transaction, volume}, author deﬁnes the forward looking time interval as:\n",
    "![Clock Definition](./fig/Jupyterfig/1.png)\n",
    "\n",
    "* **Part B:** Transaction return (Achieved)\n",
    "\n",
    "The author defines the average return during one transaction span as:\n",
    "![Clock Definition](./fig/Jupyterfig/2.png)\n",
    "\n",
    "* **Part C:** Price direction (Achieved)\n",
    "\n",
    "In order to figure out whether next price movements will be up or down, author uses average past transaction return of the stock to illustrate:\n",
    "![Clock Definition](./fig/Jupyterfig/3.png)\n",
    "\n",
    "* **Part D:** Transaction duration\n",
    "\n",
    "It may help market makers to make decisions(for example cancel quotes before hit) by measuring the amount of (calendar) time it takes to record transactions. Under M ∈ {transaction, volume}, author defines:\n",
    "![Clock Definition](./fig/Jupyterfig/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third: Predictor Variables**\n",
    "\n",
    "As the author didn't analyze problems base on point data, but interval data, he defines lookback spans for three calendar mode:\n",
    "![Clock Definition](./fig/Jupyterfig/5.png)\n",
    "\n",
    "* **Part A:** Volume and duration (Achieved)\n",
    "\n",
    "![Clock Definition](./fig/Jupyterfig/6.png)\n",
    "\n",
    "* **Part B:** Return and imbalance (Achieved)\n",
    "\n",
    "![Clock Definition](./fig/Jupyterfig/7.png)\n",
    "\n",
    "* **Part C:** Speed and cost (Achieved)\n",
    "\n",
    "![Clock Definition](./fig/Jupyterfig/8.png)\n",
    "\n",
    "So total varaibles are 117(13 * 9 lookback spans) * 3 (time clocks). Achieved 12 * 9 * 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rolling_factor(df, rely_column, boundary):\n",
    "    left_bond, right_bond = boundary\n",
    "    start_index = len(df)-sum(\n",
    "        df[rely_column]>df[rely_column].values[0]+right_bond)\n",
    "    column_list = ['breadth', 'immediacy', 'volume_all', 'volume_avg','volume_max',\n",
    "        'lambda', 'lob_imbalance', 'trn_imbalance', 'past_return',\n",
    "        'auto_cov', 'quoted_spread', 'effective_spread']\n",
    "    column_list = [f'{i}_{left_bond}_{right_bond}' for i in column_list] #/1000\n",
    "    df[column_list] = np.nan\n",
    "    for i in tqdm(range(start_index, len(df))):\n",
    "        t0 = df.loc[i, rely_column]\n",
    "        left_t0 = t0-right_bond\n",
    "        right_t0 = t0-left_bond\n",
    "        x = df[(df[rely_column]<=right_t0) & (df[rely_column]>=left_t0)]\n",
    "        df.loc[i, column_list] = calculate_factor(x, boundary)\n",
    "    return df\n",
    "\n",
    "def calculate_factor(x, boundary):\n",
    "    breadth = len(x)-1\n",
    "    if breadth > 0:\n",
    "        immediacy = (boundary[1] - boundary[0])/breadth\n",
    "        volume_all = np.nansum(x.qty.values[1:])\n",
    "        volume_avg = volume_all/breadth\n",
    "        volume_max = np.nanmax(x.qty.values[1:])\n",
    "        lambda_ = (np.nanmax(x.price.values[1:]) - np.nanmin(x.price.values[1:]))/volume_all\n",
    "        lob_imbalance = np.nanmean(((x.askqty1-x.bidqty1)/(x.askqty1+x.bidqty1)).values[1:]) if (x.askqty1+x.bidqty1).values[-1] != 0 else 0\n",
    "        x['direction'] = np.where(x['is_buyer_maker']=='BUY', 1, -1)\n",
    "        txn_imbalance = np.nansum((x.direction*x.qty).values[1:])/volume_all\n",
    "        past_return = 1 - np.nanmean(x.price.values[1:])/np.max(x.price.values[1:])\n",
    "        x['arg_max'] = x['price'].cummax().shift(1)\n",
    "        x['arg_arg_max'] = x['arg_max'].cummax()\n",
    "        auto_cov = np.nanmean((np.log(x.price/x['arg_max'])*np.log(x['arg_max']/x['arg_arg_max'])).values[1:])\n",
    "        quoted_spread = np.nanmean(((x.ask1-x.bid1)/x.price).values[1:])\n",
    "        effective_spread = np.nansum((np.log(x['arg_max']/x.price)*x.direction*x.qty*x.price).values[1:])/np.nansum((x.qty\\\n",
    "                            *x.price).values[1:])\n",
    "    else:\n",
    "        immediacy = volume_all = volume_avg = volume_max = lambda_ = lob_imbalance = txn_imbalance = past_return =\\\n",
    "    auto_cov = quoted_spread = effective_spread = 0\n",
    "    return breadth, immediacy, volume_all, volume_avg, volume_max, lambda_, lob_imbalance, txn_imbalance, past_return,\\\n",
    "    auto_cov, quoted_spread, effective_spread\n",
    "\n",
    "def rolling_response(df, rely_column, boundary):\n",
    "    left_bond, right_bond = boundary\n",
    "    end_index = len(df)-sum(\n",
    "        df[rely_column]>df[rely_column].values[-1]-right_bond)\n",
    "    column_list = ['return', 'direction']\n",
    "    column_list = [f'{i}_{right_bond}' for i in column_list]\n",
    "    df[column_list] = np.nan\n",
    "    for i in tqdm(range(1, end_index)):\n",
    "        t0 = df.loc[i, rely_column]\n",
    "        left_t0 = t0+left_bond\n",
    "        right_t0 = t0+right_bond\n",
    "        x = df[(df[rely_column]<=right_t0) & (df[rely_column]>=left_t0)]\n",
    "        return_ = np.nanmean(x.price)\n",
    "        df.loc[i, column_list[0]] = return_/df.loc[i-1, 'price']\n",
    "    for i in tqdm(range(1, end_index)):\n",
    "        t0 = df.loc[i, rely_column]\n",
    "        left_t0 = t0-right_bond\n",
    "        right_t0 = t0-left_bond\n",
    "        x = df[(df[rely_column]<right_t0) & (df[rely_column]>=left_t0)]\n",
    "        df.loc[i, column_list[1]] = 1 if df.loc[i, column_list[0]] > np.nanmean(x[column_list[0]]) else 0\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_look_back_window = [[0,1000], [1000,2000], [2000,4000], [4000, 8000], [8000, 16000], [16000, 32000], [32000, 64000], [64000, 128000], [128000, 256000]]\n",
    "transaction_look_back_window = [[i[0]/1000, i[1]/1000] for i in time_look_back_window]\n",
    "volume_look_back_window = [[i[0]/10000, i[1]/10000] for i in time_look_back_window]\n",
    "time_response_window = [[0,5000], [0, 30000]]\n",
    "transaction_response_window = [[0,10], [0, 200]]\n",
    "volume_response_window = [[0,0.5], [0, 1]]\n",
    "\n",
    "\n",
    "# time_data = merge_data(ticker, trade)\n",
    "# for i in time_response_window:\n",
    "#     time_data = rolling_response(time_data, 'time', i)\n",
    "#     time_data.to_pickle('./data/time_data.pkl')\n",
    "# for i in time_look_back_window:\n",
    "#     time_data = rolling_factor(time_data, 'time', i)\n",
    "#     time_data.to_pickle('./data/time_data.pkl')\n",
    "# print('pickled')\n",
    "\n",
    "# transaction_data = merge_data(ticker, trade)\n",
    "# for i in transaction_response_window:\n",
    "#     transaction_data = rolling_response(transaction_data, 'transaction', i)\n",
    "#     transaction_data.to_pickle('./data/transaction_data.pkl')\n",
    "# for i in transaction_look_back_window:\n",
    "#     transaction_data = rolling_factor(transaction_data, 'transaction', i)\n",
    "#     transaction_data.to_pickle('./data/transaction_data.pkl')\n",
    "# print('pickled')\n",
    "\n",
    "# volume_data = merge_data(ticker, trade)\n",
    "# for i in volume_response_window:\n",
    "#     volume_data = rolling_response(volume_data, 'volume', i)\n",
    "#     volume_data.to_pickle('./data/volume_data.pkl')\n",
    "# for i in volume_look_back_window:\n",
    "#     volume_data = rolling_factor(volume_data, 'volume', i)\n",
    "#     volume_data.to_pickle('./data/volume_data.pkl')\n",
    "# print('pickled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourth: Prediction**\n",
    "\n",
    "* **Part A:** Tuning, traing and testing (Achieved)\n",
    "\n",
    "The length of each training window is set at 5 trading days. The outer layer of rolling window consists of 40 trading days where the ﬁrst 20 days are used only for tuning hyper-parameters while the next 20 days are used for testing.\n",
    "![Clock Definition](./fig/Jupyterfig/9.png)\n",
    "\n",
    "* **Part B:** Prediction results (Partly achieved)\n",
    "1. Return predict result\n",
    "\n",
    "Every box plot represents out of sample R suquare:① RF performs better than LASSO; ② prediction becomes worse while horizon grows; ③ TxnImbalance and PastReturn play an important role in prediction 5 seconds return.\n",
    "![Clock Definition](./fig/Jupyterfig/10.png)\n",
    "![Clock Definition](./fig/Jupyterfig/11.png)\n",
    "\n",
    "2. Price direction predict result\n",
    "\n",
    "① More robust result; ② accuracy 64% for short horizon.\n",
    "![Clock Definition](./fig/Jupyterfig/12.png)\n",
    "\n",
    "3. Transaction duration prediction\n",
    "① Higher accuracy when predicting longer horizon; ② volume related features become more important; ③ more accurate than return.\n",
    "![Clock Definition](./fig/Jupyterfig/13.png)\n",
    "![Clock Definition](./fig/Jupyterfig/14.png)\n",
    "\n",
    "* **Part C:** Prediction consistency over time\n",
    "1. Results are consistent across the sample;\n",
    "2. Predictability measures are significantly positive;\n",
    "3. Volatility increasement(Covid-19) results in a slight decrease in returns and more valatility in duration.\n",
    "![Clock Definition](./fig/Jupyterfig/15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction evaluation\n",
    "def prediction_accuracy_rsquare(y, y_hat):\n",
    "    return 1-np.sum((y-y_hat)**2)/np.sum((y-np.mean(y))**2)\n",
    "\n",
    "def prediction_accuracy_rmse(y, y_hat):\n",
    "    return 1-np.mean((y-y_hat)**2)\n",
    "\n",
    "def prediction_accuracy_direction(y, y_hat):\n",
    "    return np.mean(np.where(y_hat*y>0, 1, 0))\n",
    "\n",
    "def tun_train_test(batch, x_columns, y_columns, tun_time = 3):\n",
    "    '''tun_time = train tun_time times in one tuning_period'''\n",
    "    tuning_period = len(batch)//2\n",
    "    tun_len = tuning_period // (tun_time+1)\n",
    "    tuning_ = [{'train': None, 'test': None} for i in range(tun_time)]\n",
    "\n",
    "    # split tun data\n",
    "    tuning_data = batch.iloc[:tuning_period][x_columns+y_columns].values\n",
    "    window_size = tun_len\n",
    "    num_windows = tun_time+1\n",
    "    new_shape = (num_windows, window_size, tuning_data.shape[1])\n",
    "    new_strides = (tuning_data.strides[0] * window_size, tuning_data.strides[0], tuning_data.strides[1])\n",
    "    strided_arr = stride(tuning_data, shape=new_shape, strides=new_strides)\n",
    "    for idx, x in enumerate(strided_arr):\n",
    "        if idx < tun_time:\n",
    "            tuning_[idx]['train'] = x\n",
    "        if idx != 0:\n",
    "            tuning_[idx-1]['test'] = x\n",
    "    testing_ =  batch.iloc[tuning_period-tun_len:][x_columns+y_columns].values\n",
    "    return tuning_, testing_, tun_len+1\n",
    "\n",
    "def predict_and_evaluate(train, test, apply_func, **kwargs):\n",
    "    train_x, train_y, test_x, test_y = train[:,:-1], train[:,-1], test[:,:-1], test[:,-1]\n",
    "    model = apply_func(**kwargs)\n",
    "    model.fit(train_x, train_y)\n",
    "    test_y_hat = model.predict(test_x)\n",
    "    rsquare = prediction_accuracy_rsquare(test_y, test_y_hat)\n",
    "    rmse = prediction_accuracy_rmse(test_y, test_y_hat)\n",
    "    direction_rate = prediction_accuracy_direction(test_y, test_y_hat)\n",
    "    return model, rsquare, rmse, direction_rate\n",
    "\n",
    "def grid_search(tun_batch, apply_func, grid):\n",
    "    '''grid = [{'alpha': 0.1}, {'alpha': 0.2}]'''\n",
    "    best_params = np.nan\n",
    "    best_evaluation = np.nan\n",
    "    for params in tqdm(grid):\n",
    "        evaluation = 0\n",
    "        for i in range(len(tun_batch)):\n",
    "            model, rsquare, rmse, direction_rate = predict_and_evaluate(tun_batch[i]['train'], tun_batch[i]['test'], apply_func, **params)\n",
    "            evaluation += rmse\n",
    "        evaluation = evaluation/len(tun_batch)\n",
    "        if ~(best_evaluation > evaluation): \n",
    "            best_evaluation = evaluation\n",
    "            best_params = params\n",
    "    print(f'After searching, best parameters: {best_params}, MSE: {best_evaluation}')\n",
    "    return best_params, best_evaluation\n",
    "\n",
    "def predict_backtest(batch, x_columns, y_columns, apply_func, grid):\n",
    "    tuning_data, testing_data, train_len = tun_train_test(batch, x_columns, y_columns)\n",
    "    best_params, best_evaluation = grid_search(tuning_data, apply_func, grid)\n",
    "    \n",
    "    params_importance = np.zeros((1, len(x_columns)))\n",
    "    dim0, dim1 = testing_data.shape\n",
    "    stride0, stride1 = testing_data.strides\n",
    "    stride_values = stride(testing_data, (dim0 - (train_len - 1), train_len, dim1), (stride0, stride0, stride1))\n",
    "    result_values = np.full((dim0, 1), np.nan)\n",
    "    for idx, values in enumerate(tqdm(stride_values), train_len - 1):\n",
    "        model = apply_func(**best_params)\n",
    "        train_x, train_y = values[:-1, :-1], values[:-1, -1]\n",
    "        model.fit(train_x, train_y)\n",
    "        test_y_hat = model.predict(values[-2:, :-1])[-1]\n",
    "        result_values[idx,] = test_y_hat\n",
    "        try:\n",
    "            params_importance += (model.coef_!=0).astype(int)\n",
    "        except:\n",
    "            params_importance += (model.feature_importances_!=0).astype(int)\n",
    "    params_importance = params_importance/len(stride_values)\n",
    "    rsquare = prediction_accuracy_rsquare(testing_data[train_len-1:,-1], result_values[train_len-1:])\n",
    "    rmse = prediction_accuracy_rmse(testing_data[train_len-1:,-1], result_values[train_len-1:])\n",
    "    direction_rate = prediction_accuracy_direction(testing_data[train_len-1:,-1], result_values[train_len-1:])\n",
    "    final_data = pd.DataFrame(testing_data, columns = x_columns+y_columns)\n",
    "    final_data[f'{y_columns[0]}_predict'] = result_values\n",
    "    params_importance = pd.DataFrame(params_importance, columns = x_columns, index = ['importance'])\n",
    "    return params_importance.T, rsquare, rmse, direction_rate\n",
    "\n",
    "def RF_predict(time_data, transaction_data, volume_data):\n",
    "    data = {'time': time_data.dropna(axis = 0).reset_index(drop = True)[:len(time_data)//2], \n",
    "                 'transaction': transaction_data.dropna(axis = 0).reset_index(drop = True)[:len(transaction_data)//2],\n",
    "                'volume': volume_data.dropna(axis = 0).reset_index(drop = True)[:len(volume_data)//2]}\n",
    "    del time_data, transaction_data, volume_data\n",
    "    lasso_grid = [{'alpha': 10**i, 'fit_intercept':False} for i in range(-4,6)]\n",
    "    rf_grid = [{'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': i, 'random_state':42} for i in range(3,7)]\n",
    "    return_result = pd.DataFrame(index = ['RF_MSE', 'RF_Top5ImportantFeature'])\n",
    "    direction_result = pd.DataFrame(index = ['RF_MSE', 'RF_Top5ImportantFeature'])\n",
    "    for data_type in ['time', 'transaction', 'volume']:\n",
    "        x_columns = data[data_type].columns.tolist()[14:]\n",
    "        for y_ in [10, 12]:\n",
    "            y_columns = [data[data_type].columns.tolist()[y_]]\n",
    "            data[data_type][y_columns[0]] = (data[data_type][y_columns[0]]-1)*10000\n",
    "            print(f'Calculating {y_columns[0]} for {data_type} type data:')\n",
    "            params_importance, rsquare, rmse, direction_rate = predict_backtest(data[data_type], x_columns, y_columns, RandomForestRegressor, rf_grid)\n",
    "            return_result.loc['RF_MSE', y_columns[0]] = rmse\n",
    "            return_result.loc['RF_Top5ImportantFeature', y_columns[0]] = str(params_importance.sort_values('importance').index.to_list()[-5:])\n",
    "            print(return_result.loc[['RF_MSE', 'RF_Top5ImportantFeature'], y_columns[0]])\n",
    "        for y__ in [11,13]:\n",
    "            y_columns = [data[data_type].columns.tolist()[y__]]\n",
    "            print(f'Calculating {y_columns[0]} for {data_type} type data:')\n",
    "            params_importance, rsquare, rmse, direction_rate = predict_backtest(data[data_type], x_columns, y_columns, RandomForestRegressor, rf_grid)\n",
    "            direction_result.loc['RF_MSE', y_columns[0]] = rmse\n",
    "            direction_result.loc['RF_Top5ImportantFeature', y_columns[0]] = str(params_importance.sort_values('importance').index.to_list()[-5:])\n",
    "            print(direction_result.loc[['RF_MSE', 'RF_Top5ImportantFeature'], y_columns[0]])\n",
    "    return return_result, direction_result\n",
    "\n",
    "def Lasso_predict(time_data, transaction_data, volume_data):\n",
    "    data = {'time': time_data.dropna(axis = 0).reset_index(drop = True)[:len(time_data)//2], \n",
    "                 'transaction': transaction_data.dropna(axis = 0).reset_index(drop = True)[:len(transaction_data)//2],\n",
    "                'volume': volume_data.dropna(axis = 0).reset_index(drop = True)[:len(volume_data)//2]}\n",
    "    del time_data, transaction_data, volume_data\n",
    "    lasso_grid = [{'alpha': 10**i, 'fit_intercept':False} for i in range(-4,6)]\n",
    "    rf_grid = [{'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': i, 'random_state':42} for i in range(3,7)]\n",
    "    return_result = pd.DataFrame(index = ['Lasso_MSE', 'Lasso_Top5ImportantFeature'])\n",
    "    direction_result = pd.DataFrame(index = ['Lasso_MSE', 'Lasso_Top5ImportantFeature'])\n",
    "    for data_type in ['time', 'transaction', 'volume']:\n",
    "        x_columns = data[data_type].columns.tolist()[14:]\n",
    "        for y_ in [10, 12]:\n",
    "            y_columns = [data[data_type].columns.tolist()[y_]]\n",
    "            data[data_type][y_columns[0]] = (data[data_type][y_columns[0]]-1)*10000\n",
    "            print(f'Calculating {y_columns[0]} for {data_type} type data:')\n",
    "            params_importance, rsquare, rmse, direction_rate = predict_backtest(data[data_type], x_columns, y_columns, Lasso, lasso_grid)\n",
    "            return_result.loc['Lasso_MSE', y_columns[0]] = rmse\n",
    "            return_result.loc['Lasso_Top5ImportantFeature', y_columns[0]] = str(params_importance.sort_values('importance').index.to_list()[-5:])\n",
    "            print(return_result.loc[['Lasso_MSE', 'Lasso_Top5ImportantFeature'], y_columns[0]])       \n",
    "        for y__ in [11,13]:\n",
    "            y_columns = [data[data_type].columns.tolist()[y__]]\n",
    "            print(f'Calculating {y_columns[0]} for {data_type} type data:')\n",
    "            params_importance, rsquare, rmse, direction_rate = predict_backtest(data[data_type], x_columns, y_columns, Lasso, lasso_grid)\n",
    "            direction_result.loc['Lasso_MSE', y_columns[0]] = rmse\n",
    "            direction_result.loc['Lasso_Top5ImportantFeature', y_columns[0]] = str(params_importance.sort_values('importance').index.to_list()[-5:])\n",
    "            print(direction_result.loc[['Lasso_MSE', 'Lasso_Top5ImportantFeature'], y_columns[0]])\n",
    "    return return_result, direction_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating return_5 for time type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.7005015436806293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [39:00<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.692368\n",
      "RF_Top5ImportantFeature    ['volume_avg_0.0_1.0', 'lob_imbalance_64.0_128...\n",
      "Name: return_5, dtype: object\n",
      "Calculating return_30 for time type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.11046864825747556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [38:02<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              -0.14775\n",
      "RF_Top5ImportantFeature    ['lambda_128.0_256.0', 'past_return_64.0_128.0...\n",
      "Name: return_30, dtype: object\n",
      "Calculating direction_5 for time type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'alpha': 10, 'fit_intercept': False}, MSE: 0.7378163916888809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [01:11<00:00, 132.23it/s]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.7306852408043548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [38:37<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.698033\n",
      "RF_Top5ImportantFeature    ['volume_avg_0.0_1.0', 'lob_imbalance_32.0_64....\n",
      "Name: direction_5, dtype: object\n",
      "Calculating direction_30 for time type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'alpha': 100, 'fit_intercept': False}, MSE: 0.7416326641962593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [00:23<00:00, 396.78it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 6, 'random_state': 42}, MSE: 0.6866452085199946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [57:54<00:00,  2.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.544227\n",
      "RF_Top5ImportantFeature    ['immediacy_0.0_1.0', 'breadth_0.0_1.0', 'volu...\n",
      "Name: direction_30, dtype: object\n",
      "Calculating return_10 for transaction type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.9918138153135687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [45:37<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.991834\n",
      "RF_Top5ImportantFeature    ['effective_spread_0.002_0.004', 'lob_imbalanc...\n",
      "Name: return_10, dtype: object\n",
      "Calculating return_200 for transaction type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 4, 'random_state': 42}, MSE: 0.36066737147536604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [57:48<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.457019\n",
      "RF_Top5ImportantFeature    ['immediacy_0.128_0.256', 'breadth_0.128_0.256...\n",
      "Name: return_200, dtype: object\n",
      "Calculating direction_10 for transaction type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'alpha': 0.1, 'fit_intercept': False}, MSE: 0.7624407951742039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [01:50<00:00, 86.07it/s] \n",
      "100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 6, 'random_state': 42}, MSE: 0.8250658977330505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [1:21:17<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.638578\n",
      "RF_Top5ImportantFeature    ['immediacy_0.0_0.001', 'trn_imbalance_0.0_0.0...\n",
      "Name: direction_10, dtype: object\n",
      "Calculating direction_200 for transaction type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'alpha': 10, 'fit_intercept': False}, MSE: 0.744330810815314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [00:43<00:00, 218.41it/s]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 5, 'random_state': 42}, MSE: 0.6751945419709529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [1:02:30<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.558704\n",
      "RF_Top5ImportantFeature    ['lambda_0.128_0.256', 'trn_imbalance_0.064_0....\n",
      "Name: direction_200, dtype: object\n",
      "Calculating return_0.5 for volume type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.9675975166839726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [30:15<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                               0.96593\n",
      "RF_Top5ImportantFeature    ['volume_all_0.0_0.1', 'lambda_0.0_0.1', 'effe...\n",
      "Name: return_0.5, dtype: object\n",
      "Calculating return_1 for volume type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.935797969198371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [29:56<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.928723\n",
      "RF_Top5ImportantFeature    ['past_return_0.0_0.1', 'lob_imbalance_0.0_0.1...\n",
      "Name: return_1, dtype: object\n",
      "Calculating direction_0.5 for volume type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'alpha': 100, 'fit_intercept': False}, MSE: 0.5951768634765264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [00:27<00:00, 342.42it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 3, 'random_state': 42}, MSE: 0.7306950718092158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [29:16<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_MSE                                                              0.708407\n",
      "RF_Top5ImportantFeature    ['lambda_0.0_0.1', 'lob_imbalance_1.6_3.2', 'v...\n",
      "Name: direction_0.5, dtype: object\n",
      "Calculating direction_1 for volume type data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'alpha': 100, 'fit_intercept': False}, MSE: 0.5827063860698865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9474/9474 [00:30<00:00, 315.60it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After searching, best parameters: {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 5, 'random_state': 42}, MSE: 0.7039742364536754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 6720/9474 [30:18<11:50,  3.87it/s]"
     ]
    }
   ],
   "source": [
    "time_data = pd.read_pickle('./data/time_data.pkl')\n",
    "transaction_data = pd.read_pickle('./data/transaction_data.pkl')\n",
    "volume_data = pd.read_pickle('./data/volume_data.pkl')\n",
    "# ls_return_result, ls_direction_result = Lasso_predict(time_data, transaction_data, volume_data)\n",
    "rf_return_result, rf_direction_result = RF_predict(time_data, transaction_data, volume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lasso_MSE</th>\n",
       "      <th>Lasso_Top5ImportantFeature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>return_5</th>\n",
       "      <td>0.835151</td>\n",
       "      <td>['volume_avg_1.0_2.0', 'auto_cov_128.0_256.0',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_30</th>\n",
       "      <td>0.359547</td>\n",
       "      <td>['volume_avg_1.0_2.0', 'auto_cov_128.0_256.0',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_10</th>\n",
       "      <td>0.993207</td>\n",
       "      <td>['volume_avg_0.001_0.002', 'auto_cov_0.128_0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_200</th>\n",
       "      <td>0.691657</td>\n",
       "      <td>['volume_avg_0.001_0.002', 'auto_cov_0.128_0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_0.5</th>\n",
       "      <td>0.978034</td>\n",
       "      <td>['volume_avg_0.1_0.2', 'auto_cov_12.8_25.6', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Lasso_MSE                         Lasso_Top5ImportantFeature\n",
       "return_5    0.835151  ['volume_avg_1.0_2.0', 'auto_cov_128.0_256.0',...\n",
       "return_30   0.359547  ['volume_avg_1.0_2.0', 'auto_cov_128.0_256.0',...\n",
       "return_10   0.993207  ['volume_avg_0.001_0.002', 'auto_cov_0.128_0.2...\n",
       "return_200  0.691657  ['volume_avg_0.001_0.002', 'auto_cov_0.128_0.2...\n",
       "return_0.5  0.978034  ['volume_avg_0.1_0.2', 'auto_cov_12.8_25.6', '..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ls_result = pd.concat([ls_return_result.T,ls_direction_result.T])\n",
    "# ls_result.to_csv('./data/ls_result.csv')\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "ls_result = pd.read_csv('./data/ls_result.csv')\n",
    "ls_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result = pd.concat([rf_return_result.T,rf_direction_result.T])\n",
    "rf_result.to_csv('./data/rf_result.csv')\n",
    "rf_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fifth: Cross-Sectional and Time-Series Determinants of Predictability**\n",
    "\n",
    "* **Part A:** Nominal Share Price Level and Price Discreteness: Lower price, Higher predict precision.\n",
    "* **Part B:** Stock Trading Liquidity: Higher liquidity(Lower spread or higher volume), Higher predict precision in duration, but Lower predict precision in return and direction.\n",
    "* **Part C:** Stock-Level Volatility and Jumps: Volatility(std for 15s mid price return) and Jumps(whether daily return falls into the range of 3−4%, 4−5% or greater than 5%) contribute to duration prediction, distribute to return prediction.\n",
    "* **Part D:** Asset Pricing Characteristics: betas, daily R square and daily idiosyncratic volatilitie influence on prediction\n",
    "\n",
    "**Sixth: The Value of a Millisecond**\n",
    "\n",
    "* **Part A:** The Predictability Lifespan: Only predictable before 3min, 2000 transactions, 500K volume.\n",
    "* **Part B:** The Impact of Delays in Acquiring or Processing Data: import delay to illustrate predictability decrease as data delays\n",
    "\n",
    "**Seventh: Robustness Checks**\n",
    "\n",
    "* **Part A:** Comparison and Consistency of Results Across Prediction Methods\n",
    "* **Part B:** Fine-tuning the Number of Trees in a Random Forest\n",
    "* **Part C:** Predictability Using Only Subtypes of Data: Trades vs. Quotes, transaction-only timestamp data performces well than quote-update only timestamp, for less noise in transaction data\n",
    "* **Part D:** Incremental Predictability Using Additional Data From Correlated Stocks: Additional  predictability  can  be  achieved  by  adding  data  derived  from  other  stocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
